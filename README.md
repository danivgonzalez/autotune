# autotune
Toolkit of state-of-the-art algorithms for autonomous hyper parameter tuning in neural networks

The process of tuning hyperparameters in order to optimize the training of a neural network is extremely time and resource consuming. We examine numerous automated strategies for hyperparameter tuning, including grid search, random search, hypergradient descent, Bayesian optimization, and population-based training (PBT). We implement each approach, comparing the amount of iterations each algorithm takes to converge with the train and test set accuracy. 
Keywords: hyperparameter tuning, bayesian optimization, population based train- ing, hypergradient descent, grid search, random search
